{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Smart Stock Prediction #\nJohn, a hacker has got access to database of one of Leading and most profitable trading firm of India. Database had daily share prices, buy date and sell date data from Jan, 2000 till Nov,2019. \nHe also found expected share prices for next 2 years from another table. For earning profits using this data, he started searching on web and encountered many unknown terms such as RNN, Attention Models etc. Then he remembered, he has few friends who are working on Deep learning and asked for their help. Can you help him by creating a deep learning based model for predicting buy and sell date."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Visulization ##"},{"metadata":{},"cell_type":"markdown","source":"Training Data (share_train_data.csv)\n1. Each row represent one-month data for one particular share. \n2. Share is only bought once in that month and sold in same month only. Buy date will be always before sell date.\n3. In below example, share_4 was bought on 8, Jan, 2000 and sold on 10, Jan,2000.\n\n\tShare_Name, Year, Month, Share Prices (28-31 values depending on month), Buy Date, Sell Date\n\n\t'share_4', 2000, 'JAN', 46.6, 54.7, 66.7, 44.7, 46.0, 72.8, 73.6, 37.9, 49.7, 74.3, 71.2, 51.0, 40.3, 57.8, 70.5, 66.2, 69.8, 45.8, 57.4, 50.2, 62.6, 40.6, 48.7, 71.2, 51.4, 61.4, 49.2, 52.7, 54.9, 37.3, 47.2, 8, 10\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/sharepricedateprediction/share_train_data.csv', header=None)\nprint(len(train_data))\ntrain_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[40:60]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for the first column we have different share classes lets check all the available values\nprint(len(train_data[0].unique()))\ntrain_data[0].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for the second column we have different share classes lets check all the available values\nprint(len(train_data[1].unique()))\ntrain_data[1].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for the third column we have different share classes lets check all the available values\nprint(len(train_data[2].unique()))\ntrain_data[2].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"#extracting last two columns from Traning data\n\nmonth_dict={\n    'JAN':1, 'FEB':2, 'MAR':3, 'APR':4, 'MAY':5, 'JUN':6,'JUL':7, 'AUG':8,'SEP':9, 'OCT':10, 'NOV':11, 'DEC':12\n}\n\nfrom calendar import monthrange\n\n\ndef dateExtractor(train_data):\n    buy_dates=[]\n    sell_dates=[]\n    \n    for row in train_data.values:\n        days_in_month = monthrange(int(row[1]), int(month_dict[row[2]]))[1]\n        sell_index=days_in_month+4\n        buy_index=days_in_month+3\n        buy_dates.append(row[buy_index])\n        sell_dates.append(row[sell_index])\n        \n    return buy_dates, sell_dates\n                               \nbuy_dates, sell_dates = dateExtractor(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Unique Buy Dates: '+str(set(buy_dates))+'\\nUnique Sell Date: '+str(set(sell_dates)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill all the Nan value with 0\ntrain_data=train_data.fillna(0)\ntrain_data[99:100]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our Dataset don't have same length of share price for all the records because of different days in each month, RNN will expect to have same size of records so we need to preprocess to generate consistence records."},{"metadata":{"trusted":true},"cell_type":"code","source":"def targetDateReplacer(train_data):\n    '''\n    Replace buy date and sell date with zeros so that we can generate consitent data \n    '''\n    df = pd.DataFrame(columns = [0,1,2,3,4,5,6,7,8,9,10,11,12,13, 14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35])\n    for index,row in enumerate(train_data.values):\n        days_in_month = monthrange(int(row[1]), int(month_dict[row[2]]))[1]\n        sell_index = days_in_month+4\n        buy_index = days_in_month+3\n        row[sell_index] = 0\n        row[buy_index] = 0\n        df.loc[index]=row\n    return df\n\nmodified_df = targetDateReplacer(train_data)\nmodified_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modified_df=modified_df.drop([34,35], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we need to one hot encode sharetype, year, and month column\n\nfrom sklearn.preprocessing import LabelEncoder\ndef label_encoder(modified_df):\n    cols = (0, 1, 2)\n    # process columns, apply LabelEncoder to categorical features\n    for c in cols:\n        lbl = LabelEncoder() \n        lbl.fit(list(modified_df[c].values)) \n        modified_df[c] = lbl.transform(list(modified_df[c].values))\n    \n    return modified_df\n\none_hot_df=label_encoder(modified_df)\none_hot_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one hot encoded value of first column\nmodified_df[0].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one hot encoded value of second column\nmodified_df[1].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one hot encoded value of third column\nmodified_df[2].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Normalization ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\ndef normalize_data(df):\n    min_max_scaler = preprocessing.MinMaxScaler()\n    for i in range(34):\n        df[i] = min_max_scaler.fit_transform(df[i].values.reshape(-1,1))\n    return df\n\nnorm_df=normalize_data(modified_df)\nnorm_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"norm_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we also need to normalize target values\nmin_max_scaler = preprocessing.MinMaxScaler()\nn_buy_dates = min_max_scaler.fit_transform(np.asarray(buy_dates).reshape(-1,1))\nn_sell_dates = min_max_scaler.fit_transform(np.asarray(sell_dates).reshape(-1,1))\n\n# lets create target value using list comprehension \nn_buy_dates=list(n_buy_dates)\nn_sell_dates=list(n_sell_dates)\ntarget=[[n_buy_dates[i].item(),n_sell_dates[i].item()] for i in range(len(n_sell_dates))]\ntarget[:10]","execution_count":88,"outputs":[{"output_type":"execute_result","execution_count":88,"data":{"text/plain":"[[0.06896551724137931, 0.17241379310344826],\n [0.2068965517241379, 0.7586206896551724],\n [0.4827586206896552, 0.9655172413793104],\n [0.41379310344827586, 0.5517241379310345],\n [0.7586206896551725, 0.7586206896551724],\n [0.1724137931034483, 0.9655172413793104],\n [0.3103448275862069, 0.7586206896551724],\n [0.3103448275862069, 0.689655172413793],\n [0.7241379310344828, 0.793103448275862],\n [0.5172413793103449, 0.8275862068965517]]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split_dataset into 80% training , 10% test and 10% Validation Dataset\ntrain_x=np.array(norm_df[:int(0.90*len(norm_df))])\ntrain_y=np.array(target[:int(0.90*len(target))])\nvalid_x=np.array(norm_df[int(0.90*len(norm_df)):])\nvalid_y=np.array(target[int(0.90*len(target)):])\nprint(len(train_y), len(train_x), len(valid_y), len(valid_x))","execution_count":89,"outputs":[{"output_type":"stream","text":"10970 10970 1219 1219\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = np.reshape(train_x, (train_x.shape[0], 1, train_x.shape[1]))\nvalid_x = np.reshape(valid_x, (valid_x.shape[0], 1, valid_x.shape[1]))","execution_count":97,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x.shape","execution_count":98,"outputs":[{"output_type":"execute_result","execution_count":98,"data":{"text/plain":"(10970, 1, 34)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_features=34\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM\n\nmodel = Sequential()\nmodel.add(LSTM(100, input_shape=(1, n_features)))\nmodel.add(Dense(2))\nmodel.compile(optimizer='adam', loss='mse')\nmodel.summary()","execution_count":104,"outputs":[{"output_type":"stream","text":"Model: \"sequential_9\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_7 (LSTM)                (None, 100)               54000     \n_________________________________________________________________\ndense_7 (Dense)              (None, 2)                 202       \n=================================================================\nTotal params: 54,202\nTrainable params: 54,202\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train_x, train_y, epochs=10, validation_split=0.2, batch_size=50)","execution_count":105,"outputs":[{"output_type":"stream","text":"Train on 8776 samples, validate on 2194 samples\nEpoch 1/10\n8776/8776 [==============================] - 1s 139us/step - loss: 0.0864 - val_loss: 0.0665\nEpoch 2/10\n8776/8776 [==============================] - 1s 61us/step - loss: 0.0642 - val_loss: 0.0654\nEpoch 3/10\n8776/8776 [==============================] - 1s 61us/step - loss: 0.0631 - val_loss: 0.0645\nEpoch 4/10\n8776/8776 [==============================] - 1s 63us/step - loss: 0.0628 - val_loss: 0.0655\nEpoch 5/10\n8776/8776 [==============================] - 1s 62us/step - loss: 0.0624 - val_loss: 0.0652\nEpoch 6/10\n8776/8776 [==============================] - 1s 63us/step - loss: 0.0622 - val_loss: 0.0639\nEpoch 7/10\n8776/8776 [==============================] - 1s 63us/step - loss: 0.0623 - val_loss: 0.0639\nEpoch 8/10\n8776/8776 [==============================] - 1s 62us/step - loss: 0.0620 - val_loss: 0.0653\nEpoch 9/10\n8776/8776 [==============================] - 1s 61us/step - loss: 0.0622 - val_loss: 0.0664\nEpoch 10/10\n8776/8776 [==============================] - 1s 62us/step - loss: 0.0622 - val_loss: 0.0637\n","name":"stdout"},{"output_type":"execute_result","execution_count":105,"data":{"text/plain":"<keras.callbacks.callbacks.History at 0x7f4ccc069fd0>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Pytorch ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\n#create Tensor Dataset\ntrain_data=TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\nvalid_data=TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n\n#dataloader\nbatch_size=50\ntrain_loader=DataLoader(train_data, batch_size=batch_size, shuffle=True)\nvalid_loader=DataLoader(valid_data, batch_size=batch_size, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# obtain one batch of training data\ndataiter = iter(train_loader)\nsample_x, sample_y = dataiter.next()\nprint('Sample input size: ', sample_x.size()) # batch_size, seq_length\nprint('Sample input: \\n', sample_x)\nprint()\nprint('Sample label size: ', sample_y.size()) # batch_size\nprint('Sample label: \\n', sample_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\n\nclass RNN(nn.Module):\n    \"\"\"\n    The RNN model that will be used to perform Sentiment analysis.\n    \"\"\"\n\n    def __init__(self, output_size, input_dim, hidden_dim, n_layers, drop_prob=0.5):\n        \"\"\"\n        Initialize the model by setting up the layers.\n        \"\"\"\n        super(RNN, self).__init__()\n\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        self.input_dim = input_dim\n        \n        # embedding and LSTM layers\n        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, \n                            dropout=drop_prob, batch_first=True)\n        \n        # dropout layer\n        self.dropout = nn.Dropout(0.3)\n        \n        # linear and sigmoid layers\n        self.fc1 = nn.Linear(hidden_dim, 256)\n        self.fc2 = nn.Linear(256, 32)\n        self.fc3 = nn.Linear(32, output_size)\n        self.sig = nn.Sigmoid()\n        \n\n    def forward(self, x, hidden):\n        \"\"\"\n        Perform a forward pass of our model on some input and hidden state.\n        \"\"\"\n        \n        \n        batch_size = x.size(0)\n        print(batch_size)\n        \n        x = x.unsqueeze(0)\n        print(x, x.shape)\n        \n        lstm_out, hidden = self.lstm(x, hidden)\n    \n        # stack up lstm outputs\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        # dropout and fully-connected layer\n        out = self.dropout(lstm_out)\n        out = self.fc1(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n\n        \n        # return last sigmoid output and hidden state\n        return out, hidden\n    \n    \n    def init_hidden(self, batch_size):\n        ''' Initializes hidden state '''\n        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n        # initialized to zero, for hidden state and cell state of LSTM\n        weight = next(self.parameters()).data\n        \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n        \n        return hidden","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate the model w/ hyperparams\noutput_size = 2\ninput_dim = 34\nhidden_dim = 100\nn_layers = 2\n\nnet = RNN(output_size, input_dim, hidden_dim, n_layers)\n\nprint(net)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(list(net.parameters()))):\n    print(list(net.parameters())[i].size())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loss and optimization functions\nlr=0.001\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First checking if GPU is available\ntrain_on_gpu=torch.cuda.is_available()\n\nif(train_on_gpu):\n    print('Training on GPU.')\nelse:\n    print('No GPU available, training on CPU.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training params\n\nepochs = 2 # 3-4 is approx where I noticed the validation loss stop decreasing\n\ncounter = 0\nprint_every = 100\nclip=5 # gradient clipping\n# move model to GPU, if available\nif(train_on_gpu):\n    net.cuda()\n\nnet.train()\n# train for some number of epochs\nfor e in range(epochs):\n    # initialize hidden state\n    h = net.init_hidden(50)\n    # batch loop\n    for inputs, labels in train_loader:\n        counter += 1    \n        if(train_on_gpu):\n            inputs, labels = inputs.cuda(), labels.cuda()\n        \n        # Creating new variables for the hidden state, otherwise\n        # we'd backprop through the entire training history\n        h = tuple([each.data for each in h])\n        # zero accumulated gradients\n        net.zero_grad()\n\n        # get the output from the model\n        output, h = net(inputs, h)\n\n        # calculate the loss and perform backprop\n        loss = criterion(output.squeeze(), labels.float())\n        loss.backward()\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n        nn.utils.clip_grad_norm_(net.parameters(), clip)\n        optimizer.step()\n\n        # loss stats\n        if counter % print_every == 0:\n            # Get validation loss\n            val_h = net.init_hidden(batch_size)\n            val_losses = []\n            net.eval()\n            for inputs, labels in valid_loader:\n                # Creating new variables for the hidden state, otherwise\n                # we'd backprop through the entire training history\n                val_h = tuple([each.data for each in val_h])\n                if(inputs.shape[0] != batch_size):\n                    continue\n                if(train_on_gpu):\n                    inputs, labels = inputs.cuda(), labels.cuda()\n\n                output, val_h = net(inputs, val_h)\n                val_loss = criterion(output.squeeze(), labels.float())\n\n                val_losses.append(val_loss.item())\n\n            net.train()\n            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n                  \"Step: {}...\".format(counter),\n                  \"Loss: {:.6f}...\".format(loss.item()),\n                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":93,"outputs":[{"output_type":"stream","text":"Model: \"sequential_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_3 (LSTM)                (None, 100)               54000     \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 101       \n=================================================================\nTotal params: 54,101\nTrainable params: 54,101\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":87,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Error when checking input: expected lstm_2_input to have 3 dimensions, but got array with shape (10970, 34)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-87-9dfc21002c2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    133\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Error when checking input: expected lstm_2_input to have 3 dimensions, but got array with shape (10970, 34)"]}]},{"metadata":{},"cell_type":"markdown","source":"Testing Data (share_test_data.csv)\n1. Predict buy and sell date for estimated shares prices for next 2 years.\n2. You can only buy once in month and sell once in month for given share.\n3. Testing data have same format as training data except buy and sell date are not present."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv('/kaggle/input/sharepricedateprediction/share_test_data.csv', header=None)\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"en_test=label_encoder(test_data)\nnorm_data=normalize_data(en_test)\nnorm_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Expected Output\n1. Python code for training model using given train data using Tensorflow, Pytorch or Keras frameworks.\n2. Python code for predicting on test data using trained model.\n3. Generated predictions for test data in same format as training data. \n4. A PDF report explaining your approach and results. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}