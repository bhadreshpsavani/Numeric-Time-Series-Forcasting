{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Smart Stock Prediction #\nJohn, a hacker has got access to database of one of Leading and most profitable trading firm of India. Database had daily share prices, buy date and sell date data from Jan, 2000 till Nov,2019. \nHe also found expected share prices for next 2 years from another table. For earning profits using this data, he started searching on web and encountered many unknown terms such as RNN, Attention Models etc. Then he remembered, he has few friends who are working on Deep learning and asked for their help. Can you help him by creating a deep learning based model for predicting buy and sell date."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":106,"outputs":[{"output_type":"stream","text":"/kaggle/input/sharepricedateprediction/Smart Stock Prediction.docx\n/kaggle/input/sharepricedateprediction/share_train_data.csv\n/kaggle/input/sharepricedateprediction/share_test_data.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Data Visulization ##"},{"metadata":{},"cell_type":"markdown","source":"Training Data (share_train_data.csv)\n1. Each row represent one-month data for one particular share. \n2. Share is only bought once in that month and sold in same month only. Buy date will be always before sell date.\n3. In below example, share_4 was bought on 8, Jan, 2000 and sold on 10, Jan,2000.\n\n\tShare_Name, Year, Month, Share Prices (28-31 values depending on month), Buy Date, Sell Date\n\n\t'share_4', 2000, 'JAN', 46.6, 54.7, 66.7, 44.7, 46.0, 72.8, 73.6, 37.9, 49.7, 74.3, 71.2, 51.0, 40.3, 57.8, 70.5, 66.2, 69.8, 45.8, 57.4, 50.2, 62.6, 40.6, 48.7, 71.2, 51.4, 61.4, 49.2, 52.7, 54.9, 37.3, 47.2, 8, 10\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/sharepricedateprediction/share_train_data.csv', header=None)\nprint(len(train_data))\ntrain_data.head(10)","execution_count":107,"outputs":[{"output_type":"stream","text":"12189\n","name":"stdout"},{"output_type":"execute_result","execution_count":107,"data":{"text/plain":"         0     1    2      3      4      5      6      7      8      9   ...  \\\n0   share_1  2000  JAN  345.0  376.4  344.4  361.7  347.5  350.2  383.8  ...   \n1   share_2  2000  JAN  114.6   87.2  114.3   85.6  107.9   94.3   81.6  ...   \n2   share_3  2000  JAN  286.3  271.4  272.9  298.4  265.1  266.3  274.4  ...   \n3   share_4  2000  JAN  278.9  263.3  256.2  287.8  280.0  267.2  272.5  ...   \n4   share_5  2000  JAN  154.4  163.1  136.9  138.9  145.5  158.3  165.5  ...   \n5   share_6  2000  JAN  338.5  321.0  310.4  332.8  324.8  299.6  313.1  ...   \n6   share_7  2000  JAN  200.2  224.2  232.6  203.7  214.8  206.3  219.8  ...   \n7   share_8  2000  JAN  401.1  415.2  408.6  399.8  383.8  396.8  398.8  ...   \n8   share_9  2000  JAN   76.8   57.6   74.9   55.6   77.0   75.6   60.3  ...   \n9  share_10  2000  JAN  435.2  452.2  447.7  437.3  424.4  454.9  439.5  ...   \n\n      26     27     28     29     30     31     32     33    34    35  \n0  358.6  383.5  366.9  373.7  367.8  347.0  379.0  352.8   3.0   7.0  \n1  116.1  102.0   94.9   93.4   98.7  112.6   92.5  113.6   7.0  24.0  \n2  272.6  270.5  281.0  276.3  287.8  289.2  300.8  293.7  15.0  30.0  \n3  260.0  284.1  281.0  274.4  274.9  272.5  269.3  255.2  13.0  18.0  \n4  167.0  137.1  163.1  135.6  158.5  148.0  136.2  162.8  23.0  24.0  \n5  338.8  303.4  326.7  306.1  301.7  301.7  339.2  323.1   6.0  30.0  \n6  231.3  194.2  217.4  200.0  227.3  205.8  213.8  200.7  10.0  24.0  \n7  413.8  411.9  386.0  413.2  407.9  396.1  406.8  410.1  10.0  22.0  \n8   85.2   86.1   73.1   67.4   66.9   63.9   49.5   64.9  22.0  25.0  \n9  431.1  422.7  454.4  431.9  423.5  434.0  443.3  442.0  16.0  26.0  \n\n[10 rows x 36 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>34</th>\n      <th>35</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>share_1</td>\n      <td>2000</td>\n      <td>JAN</td>\n      <td>345.0</td>\n      <td>376.4</td>\n      <td>344.4</td>\n      <td>361.7</td>\n      <td>347.5</td>\n      <td>350.2</td>\n      <td>383.8</td>\n      <td>...</td>\n      <td>358.6</td>\n      <td>383.5</td>\n      <td>366.9</td>\n      <td>373.7</td>\n      <td>367.8</td>\n      <td>347.0</td>\n      <td>379.0</td>\n      <td>352.8</td>\n      <td>3.0</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>share_2</td>\n      <td>2000</td>\n      <td>JAN</td>\n      <td>114.6</td>\n      <td>87.2</td>\n      <td>114.3</td>\n      <td>85.6</td>\n      <td>107.9</td>\n      <td>94.3</td>\n      <td>81.6</td>\n      <td>...</td>\n      <td>116.1</td>\n      <td>102.0</td>\n      <td>94.9</td>\n      <td>93.4</td>\n      <td>98.7</td>\n      <td>112.6</td>\n      <td>92.5</td>\n      <td>113.6</td>\n      <td>7.0</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>share_3</td>\n      <td>2000</td>\n      <td>JAN</td>\n      <td>286.3</td>\n      <td>271.4</td>\n      <td>272.9</td>\n      <td>298.4</td>\n      <td>265.1</td>\n      <td>266.3</td>\n      <td>274.4</td>\n      <td>...</td>\n      <td>272.6</td>\n      <td>270.5</td>\n      <td>281.0</td>\n      <td>276.3</td>\n      <td>287.8</td>\n      <td>289.2</td>\n      <td>300.8</td>\n      <td>293.7</td>\n      <td>15.0</td>\n      <td>30.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>share_4</td>\n      <td>2000</td>\n      <td>JAN</td>\n      <td>278.9</td>\n      <td>263.3</td>\n      <td>256.2</td>\n      <td>287.8</td>\n      <td>280.0</td>\n      <td>267.2</td>\n      <td>272.5</td>\n      <td>...</td>\n      <td>260.0</td>\n      <td>284.1</td>\n      <td>281.0</td>\n      <td>274.4</td>\n      <td>274.9</td>\n      <td>272.5</td>\n      <td>269.3</td>\n      <td>255.2</td>\n      <td>13.0</td>\n      <td>18.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>share_5</td>\n      <td>2000</td>\n      <td>JAN</td>\n      <td>154.4</td>\n      <td>163.1</td>\n      <td>136.9</td>\n      <td>138.9</td>\n      <td>145.5</td>\n      <td>158.3</td>\n      <td>165.5</td>\n      <td>...</td>\n      <td>167.0</td>\n      <td>137.1</td>\n      <td>163.1</td>\n      <td>135.6</td>\n      <td>158.5</td>\n      <td>148.0</td>\n      <td>136.2</td>\n      <td>162.8</td>\n      <td>23.0</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>share_6</td>\n      <td>2000</td>\n      <td>JAN</td>\n      <td>338.5</td>\n      <td>321.0</td>\n      <td>310.4</td>\n      <td>332.8</td>\n      <td>324.8</td>\n      <td>299.6</td>\n      <td>313.1</td>\n      <td>...</td>\n      <td>338.8</td>\n      <td>303.4</td>\n      <td>326.7</td>\n      <td>306.1</td>\n      <td>301.7</td>\n      <td>301.7</td>\n      <td>339.2</td>\n      <td>323.1</td>\n      <td>6.0</td>\n      <td>30.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>share_7</td>\n      <td>2000</td>\n      <td>JAN</td>\n      <td>200.2</td>\n      <td>224.2</td>\n      <td>232.6</td>\n      <td>203.7</td>\n      <td>214.8</td>\n      <td>206.3</td>\n      <td>219.8</td>\n      <td>...</td>\n      <td>231.3</td>\n      <td>194.2</td>\n      <td>217.4</td>\n      <td>200.0</td>\n      <td>227.3</td>\n      <td>205.8</td>\n      <td>213.8</td>\n      <td>200.7</td>\n      <td>10.0</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>share_8</td>\n      <td>2000</td>\n      <td>JAN</td>\n      <td>401.1</td>\n      <td>415.2</td>\n      <td>408.6</td>\n      <td>399.8</td>\n      <td>383.8</td>\n      <td>396.8</td>\n      <td>398.8</td>\n      <td>...</td>\n      <td>413.8</td>\n      <td>411.9</td>\n      <td>386.0</td>\n      <td>413.2</td>\n      <td>407.9</td>\n      <td>396.1</td>\n      <td>406.8</td>\n      <td>410.1</td>\n      <td>10.0</td>\n      <td>22.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>share_9</td>\n      <td>2000</td>\n      <td>JAN</td>\n      <td>76.8</td>\n      <td>57.6</td>\n      <td>74.9</td>\n      <td>55.6</td>\n      <td>77.0</td>\n      <td>75.6</td>\n      <td>60.3</td>\n      <td>...</td>\n      <td>85.2</td>\n      <td>86.1</td>\n      <td>73.1</td>\n      <td>67.4</td>\n      <td>66.9</td>\n      <td>63.9</td>\n      <td>49.5</td>\n      <td>64.9</td>\n      <td>22.0</td>\n      <td>25.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>share_10</td>\n      <td>2000</td>\n      <td>JAN</td>\n      <td>435.2</td>\n      <td>452.2</td>\n      <td>447.7</td>\n      <td>437.3</td>\n      <td>424.4</td>\n      <td>454.9</td>\n      <td>439.5</td>\n      <td>...</td>\n      <td>431.1</td>\n      <td>422.7</td>\n      <td>454.4</td>\n      <td>431.9</td>\n      <td>423.5</td>\n      <td>434.0</td>\n      <td>443.3</td>\n      <td>442.0</td>\n      <td>16.0</td>\n      <td>26.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 36 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[40:60]","execution_count":108,"outputs":[{"output_type":"execute_result","execution_count":108,"data":{"text/plain":"          0     1    2      3      4      5      6      7      8      9   ...  \\\n40  share_41  2000  JAN  149.1  137.4  153.7  129.8  128.1  144.0  144.0  ...   \n41  share_42  2000  JAN   99.4   87.0   92.8  103.1  120.9   86.7  107.1  ...   \n42  share_43  2000  JAN  460.1  459.0  438.9  468.4  459.8  440.2  452.3  ...   \n43  share_44  2000  JAN  324.3  314.0  320.7  336.7  312.0  337.8  315.4  ...   \n44  share_45  2000  JAN  142.6  128.6  150.8  142.5  137.0  161.6  159.4  ...   \n45  share_46  2000  JAN  450.9  430.5  430.2  428.5  435.6  432.0  451.3  ...   \n46  share_47  2000  JAN  500.5  475.3  491.6  501.7  500.6  495.1  498.3  ...   \n47  share_48  2000  JAN  257.2  231.9  265.1  261.5  255.9  251.7  231.7  ...   \n48  share_49  2000  JAN  247.7  214.1  235.9  243.3  235.4  228.3  217.4  ...   \n49  share_50  2000  JAN  168.3  149.0  184.3  180.7  183.8  175.0  171.1  ...   \n50  share_51  2000  JAN   52.2   43.3   62.1   62.0   46.7   62.8   59.6  ...   \n51   share_1  2000  FEB  366.2  368.4  368.0  357.7  345.3  362.4  357.5  ...   \n52   share_2  2000  FEB  113.3   94.8  116.6  111.4   91.1   91.4   93.0  ...   \n53   share_3  2000  FEB  291.5  286.2  270.4  282.5  299.4  272.8  269.9  ...   \n54   share_4  2000  FEB  281.7  284.6  267.2  286.4  280.7  253.1  256.0  ...   \n55   share_5  2000  FEB  147.6  160.2  138.3  152.7  151.7  140.1  140.9  ...   \n56   share_6  2000  FEB  302.6  335.6  333.3  331.3  322.8  328.9  306.5  ...   \n57   share_7  2000  FEB  193.7  217.2  223.9  201.4  225.9  196.2  230.7  ...   \n58   share_8  2000  FEB  415.4  384.6  385.8  401.1  397.3  383.1  409.1  ...   \n59   share_9  2000  FEB   84.7   68.7   60.3   62.4   80.2   61.2   64.0  ...   \n\n       26     27     28     29     30     31     32     33    34    35  \n40  136.6  145.7  127.1  156.1  149.0  146.7  152.3  152.6  12.0  14.0  \n41   90.5   89.4  108.2  114.0  122.2   96.1   88.8  111.8   6.0  28.0  \n42  464.2  438.1  450.1  463.3  460.0  476.4  440.4  464.8  25.0  29.0  \n43  308.0  317.9  327.4  308.4  311.3  334.6  310.6  320.9  10.0  15.0  \n44  161.9  164.0  137.1  156.2  145.3  152.8  132.3  151.6   2.0  11.0  \n45  459.7  447.8  421.8  442.7  434.8  427.1  445.6  453.8  22.0  24.0  \n46  470.4  471.0  486.9  498.3  493.5  482.4  472.1  483.7  16.0  17.0  \n47  256.0  243.3  256.4  242.7  228.7  242.0  244.2  253.2   9.0  22.0  \n48  220.6  229.5  216.3  217.1  248.0  239.7  225.8  240.7   2.0  28.0  \n49  170.7  149.3  164.2  150.2  161.1  184.7  146.3  165.3  22.0  29.0  \n50   37.4   68.1   42.5   67.9   39.9   51.6   49.5   76.0  24.0  31.0  \n51  346.1  350.8  351.7  369.9  345.2  381.7   12.0   19.0   NaN   NaN  \n52   78.9   85.6   87.3   86.7  115.2   89.5   24.0   28.0   NaN   NaN  \n53  294.9  270.7  283.9  282.8  299.4  285.7   20.0   28.0   NaN   NaN  \n54  260.1  271.2  255.7  282.6  268.9  262.3    6.0   15.0   NaN   NaN  \n55  149.2  152.9  147.8  166.9  153.9  164.6   10.0   17.0   NaN   NaN  \n56  324.3  303.3  302.9  313.6  304.5  318.9    1.0   21.0   NaN   NaN  \n57  231.8  229.2  223.5  205.5  207.9  206.8    1.0   11.0   NaN   NaN  \n58  413.2  413.9  381.8  379.3  405.1  389.8   17.0   25.0   NaN   NaN  \n59   71.1   82.1   53.2   48.8   83.5   72.6   15.0   23.0   NaN   NaN  \n\n[20 rows x 36 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>34</th>\n      <th>35</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>40</th>\n      <td>share_41</td>\n      <td>2000</td>\n      <td>JAN</td>\n      <td>149.1</td>\n      <td>137.4</td>\n      <td>153.7</td>\n      <td>129.8</td>\n      <td>128.1</td>\n      <td>144.0</td>\n      <td>144.0</td>\n      <td>...</td>\n      <td>136.6</td>\n      <td>145.7</td>\n      <td>127.1</td>\n      <td>156.1</td>\n      <td>149.0</td>\n      <td>146.7</td>\n      <td>152.3</td>\n      <td>152.6</td>\n      <td>12.0</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>share_42</td>\n      <td>2000</td>\n      <td>JAN</td>\n      <td>99.4</td>\n      <td>87.0</td>\n      <td>92.8</td>\n      <td>103.1</td>\n      <td>120.9</td>\n      <td>86.7</td>\n      <td>107.1</td>\n      <td>...</td>\n      <td>90.5</td>\n      <td>89.4</td>\n      <td>108.2</td>\n      <td>114.0</td>\n      <td>122.2</td>\n      <td>96.1</td>\n      <td>88.8</td>\n      <td>111.8</td>\n      <td>6.0</td>\n      <td>28.0</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>share_43</td>\n      <td>2000</td>\n      <td>JAN</td>\n      <td>460.1</td>\n      <td>459.0</td>\n      <td>438.9</td>\n      <td>468.4</td>\n      <td>459.8</td>\n      <td>440.2</td>\n      <td>452.3</td>\n      <td>...</td>\n      <td>464.2</td>\n      <td>438.1</td>\n      <td>450.1</td>\n      <td>463.3</td>\n      <td>460.0</td>\n      <td>476.4</td>\n      <td>440.4</td>\n      <td>464.8</td>\n      <td>25.0</td>\n      <td>29.0</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>share_44</td>\n      <td>2000</td>\n      <td>JAN</td>\n      <td>324.3</td>\n      <td>314.0</td>\n      <td>320.7</td>\n      <td>336.7</td>\n      <td>312.0</td>\n      <td>337.8</td>\n      <td>315.4</td>\n      <td>...</td>\n      <td>308.0</td>\n      <td>317.9</td>\n      <td>327.4</td>\n      <td>308.4</td>\n      <td>311.3</td>\n      <td>334.6</td>\n      <td>310.6</td>\n      <td>320.9</td>\n      <td>10.0</td>\n      <td>15.0</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>share_45</td>\n      <td>2000</td>\n      <td>JAN</td>\n      <td>142.6</td>\n      <td>128.6</td>\n      <td>150.8</td>\n      <td>142.5</td>\n      <td>137.0</td>\n      <td>161.6</td>\n      <td>159.4</td>\n      <td>...</td>\n      <td>161.9</td>\n      <td>164.0</td>\n      <td>137.1</td>\n      <td>156.2</td>\n      <td>145.3</td>\n      <td>152.8</td>\n      <td>132.3</td>\n      <td>151.6</td>\n      <td>2.0</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>share_46</td>\n      <td>2000</td>\n      <td>JAN</td>\n      <td>450.9</td>\n      <td>430.5</td>\n      <td>430.2</td>\n      <td>428.5</td>\n      <td>435.6</td>\n      <td>432.0</td>\n      <td>451.3</td>\n      <td>...</td>\n      <td>459.7</td>\n      <td>447.8</td>\n      <td>421.8</td>\n      <td>442.7</td>\n      <td>434.8</td>\n      <td>427.1</td>\n      <td>445.6</td>\n      <td>453.8</td>\n      <td>22.0</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>share_47</td>\n      <td>2000</td>\n      <td>JAN</td>\n      <td>500.5</td>\n      <td>475.3</td>\n      <td>491.6</td>\n      <td>501.7</td>\n      <td>500.6</td>\n      <td>495.1</td>\n      <td>498.3</td>\n      <td>...</td>\n      <td>470.4</td>\n      <td>471.0</td>\n      <td>486.9</td>\n      <td>498.3</td>\n      <td>493.5</td>\n      <td>482.4</td>\n      <td>472.1</td>\n      <td>483.7</td>\n      <td>16.0</td>\n      <td>17.0</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>share_48</td>\n      <td>2000</td>\n      <td>JAN</td>\n      <td>257.2</td>\n      <td>231.9</td>\n      <td>265.1</td>\n      <td>261.5</td>\n      <td>255.9</td>\n      <td>251.7</td>\n      <td>231.7</td>\n      <td>...</td>\n      <td>256.0</td>\n      <td>243.3</td>\n      <td>256.4</td>\n      <td>242.7</td>\n      <td>228.7</td>\n      <td>242.0</td>\n      <td>244.2</td>\n      <td>253.2</td>\n      <td>9.0</td>\n      <td>22.0</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>share_49</td>\n      <td>2000</td>\n      <td>JAN</td>\n      <td>247.7</td>\n      <td>214.1</td>\n      <td>235.9</td>\n      <td>243.3</td>\n      <td>235.4</td>\n      <td>228.3</td>\n      <td>217.4</td>\n      <td>...</td>\n      <td>220.6</td>\n      <td>229.5</td>\n      <td>216.3</td>\n      <td>217.1</td>\n      <td>248.0</td>\n      <td>239.7</td>\n      <td>225.8</td>\n      <td>240.7</td>\n      <td>2.0</td>\n      <td>28.0</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>share_50</td>\n      <td>2000</td>\n      <td>JAN</td>\n      <td>168.3</td>\n      <td>149.0</td>\n      <td>184.3</td>\n      <td>180.7</td>\n      <td>183.8</td>\n      <td>175.0</td>\n      <td>171.1</td>\n      <td>...</td>\n      <td>170.7</td>\n      <td>149.3</td>\n      <td>164.2</td>\n      <td>150.2</td>\n      <td>161.1</td>\n      <td>184.7</td>\n      <td>146.3</td>\n      <td>165.3</td>\n      <td>22.0</td>\n      <td>29.0</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>share_51</td>\n      <td>2000</td>\n      <td>JAN</td>\n      <td>52.2</td>\n      <td>43.3</td>\n      <td>62.1</td>\n      <td>62.0</td>\n      <td>46.7</td>\n      <td>62.8</td>\n      <td>59.6</td>\n      <td>...</td>\n      <td>37.4</td>\n      <td>68.1</td>\n      <td>42.5</td>\n      <td>67.9</td>\n      <td>39.9</td>\n      <td>51.6</td>\n      <td>49.5</td>\n      <td>76.0</td>\n      <td>24.0</td>\n      <td>31.0</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>share_1</td>\n      <td>2000</td>\n      <td>FEB</td>\n      <td>366.2</td>\n      <td>368.4</td>\n      <td>368.0</td>\n      <td>357.7</td>\n      <td>345.3</td>\n      <td>362.4</td>\n      <td>357.5</td>\n      <td>...</td>\n      <td>346.1</td>\n      <td>350.8</td>\n      <td>351.7</td>\n      <td>369.9</td>\n      <td>345.2</td>\n      <td>381.7</td>\n      <td>12.0</td>\n      <td>19.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>share_2</td>\n      <td>2000</td>\n      <td>FEB</td>\n      <td>113.3</td>\n      <td>94.8</td>\n      <td>116.6</td>\n      <td>111.4</td>\n      <td>91.1</td>\n      <td>91.4</td>\n      <td>93.0</td>\n      <td>...</td>\n      <td>78.9</td>\n      <td>85.6</td>\n      <td>87.3</td>\n      <td>86.7</td>\n      <td>115.2</td>\n      <td>89.5</td>\n      <td>24.0</td>\n      <td>28.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>share_3</td>\n      <td>2000</td>\n      <td>FEB</td>\n      <td>291.5</td>\n      <td>286.2</td>\n      <td>270.4</td>\n      <td>282.5</td>\n      <td>299.4</td>\n      <td>272.8</td>\n      <td>269.9</td>\n      <td>...</td>\n      <td>294.9</td>\n      <td>270.7</td>\n      <td>283.9</td>\n      <td>282.8</td>\n      <td>299.4</td>\n      <td>285.7</td>\n      <td>20.0</td>\n      <td>28.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>share_4</td>\n      <td>2000</td>\n      <td>FEB</td>\n      <td>281.7</td>\n      <td>284.6</td>\n      <td>267.2</td>\n      <td>286.4</td>\n      <td>280.7</td>\n      <td>253.1</td>\n      <td>256.0</td>\n      <td>...</td>\n      <td>260.1</td>\n      <td>271.2</td>\n      <td>255.7</td>\n      <td>282.6</td>\n      <td>268.9</td>\n      <td>262.3</td>\n      <td>6.0</td>\n      <td>15.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>share_5</td>\n      <td>2000</td>\n      <td>FEB</td>\n      <td>147.6</td>\n      <td>160.2</td>\n      <td>138.3</td>\n      <td>152.7</td>\n      <td>151.7</td>\n      <td>140.1</td>\n      <td>140.9</td>\n      <td>...</td>\n      <td>149.2</td>\n      <td>152.9</td>\n      <td>147.8</td>\n      <td>166.9</td>\n      <td>153.9</td>\n      <td>164.6</td>\n      <td>10.0</td>\n      <td>17.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>share_6</td>\n      <td>2000</td>\n      <td>FEB</td>\n      <td>302.6</td>\n      <td>335.6</td>\n      <td>333.3</td>\n      <td>331.3</td>\n      <td>322.8</td>\n      <td>328.9</td>\n      <td>306.5</td>\n      <td>...</td>\n      <td>324.3</td>\n      <td>303.3</td>\n      <td>302.9</td>\n      <td>313.6</td>\n      <td>304.5</td>\n      <td>318.9</td>\n      <td>1.0</td>\n      <td>21.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>share_7</td>\n      <td>2000</td>\n      <td>FEB</td>\n      <td>193.7</td>\n      <td>217.2</td>\n      <td>223.9</td>\n      <td>201.4</td>\n      <td>225.9</td>\n      <td>196.2</td>\n      <td>230.7</td>\n      <td>...</td>\n      <td>231.8</td>\n      <td>229.2</td>\n      <td>223.5</td>\n      <td>205.5</td>\n      <td>207.9</td>\n      <td>206.8</td>\n      <td>1.0</td>\n      <td>11.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>share_8</td>\n      <td>2000</td>\n      <td>FEB</td>\n      <td>415.4</td>\n      <td>384.6</td>\n      <td>385.8</td>\n      <td>401.1</td>\n      <td>397.3</td>\n      <td>383.1</td>\n      <td>409.1</td>\n      <td>...</td>\n      <td>413.2</td>\n      <td>413.9</td>\n      <td>381.8</td>\n      <td>379.3</td>\n      <td>405.1</td>\n      <td>389.8</td>\n      <td>17.0</td>\n      <td>25.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>share_9</td>\n      <td>2000</td>\n      <td>FEB</td>\n      <td>84.7</td>\n      <td>68.7</td>\n      <td>60.3</td>\n      <td>62.4</td>\n      <td>80.2</td>\n      <td>61.2</td>\n      <td>64.0</td>\n      <td>...</td>\n      <td>71.1</td>\n      <td>82.1</td>\n      <td>53.2</td>\n      <td>48.8</td>\n      <td>83.5</td>\n      <td>72.6</td>\n      <td>15.0</td>\n      <td>23.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>20 rows × 36 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for the first column we have different share classes lets check all the available values\nprint(len(train_data[0].unique()))\ntrain_data[0].unique()","execution_count":109,"outputs":[{"output_type":"stream","text":"51\n","name":"stdout"},{"output_type":"execute_result","execution_count":109,"data":{"text/plain":"array(['share_1', 'share_2', 'share_3', 'share_4', 'share_5', 'share_6',\n       'share_7', 'share_8', 'share_9', 'share_10', 'share_11',\n       'share_12', 'share_13', 'share_14', 'share_15', 'share_16',\n       'share_17', 'share_18', 'share_19', 'share_20', 'share_21',\n       'share_22', 'share_23', 'share_24', 'share_25', 'share_26',\n       'share_27', 'share_28', 'share_29', 'share_30', 'share_31',\n       'share_32', 'share_33', 'share_34', 'share_35', 'share_36',\n       'share_37', 'share_38', 'share_39', 'share_40', 'share_41',\n       'share_42', 'share_43', 'share_44', 'share_45', 'share_46',\n       'share_47', 'share_48', 'share_49', 'share_50', 'share_51'],\n      dtype=object)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for the second column we have different share classes lets check all the available values\nprint(len(train_data[1].unique()))\ntrain_data[1].unique()","execution_count":110,"outputs":[{"output_type":"stream","text":"20\n","name":"stdout"},{"output_type":"execute_result","execution_count":110,"data":{"text/plain":"array([2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010,\n       2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for the third column we have different share classes lets check all the available values\nprint(len(train_data[2].unique()))\ntrain_data[2].unique()","execution_count":111,"outputs":[{"output_type":"stream","text":"12\n","name":"stdout"},{"output_type":"execute_result","execution_count":111,"data":{"text/plain":"array(['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP',\n       'OCT', 'NOV', 'DEC'], dtype=object)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"#extracting last two columns from Traning data\n\nmonth_dict={\n    'JAN':1, 'FEB':2, 'MAR':3, 'APR':4, 'MAY':5, 'JUN':6,'JUL':7, 'AUG':8,'SEP':9, 'OCT':10, 'NOV':11, 'DEC':12\n}\n\nfrom calendar import monthrange\n\n\ndef dateExtractor(train_data):\n    buy_dates=[]\n    sell_dates=[]\n    \n    for row in train_data.values:\n        days_in_month = monthrange(int(row[1]), int(month_dict[row[2]]))[1]\n        sell_index=days_in_month+4\n        buy_index=days_in_month+3\n        buy_dates.append(row[buy_index])\n        sell_dates.append(row[sell_index])\n        \n    return buy_dates, sell_dates\n                               \nbuy_dates, sell_dates = dateExtractor(train_data)","execution_count":112,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Unique Buy Dates: '+str(set(buy_dates))+'\\nUnique Sell Date: '+str(set(sell_dates)))","execution_count":113,"outputs":[{"output_type":"stream","text":"Unique Buy Dates: {1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0}\nUnique Sell Date: {2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0}\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill all the Nan value with 0\ntrain_data=train_data.fillna(0)\ntrain_data[99:100]","execution_count":114,"outputs":[{"output_type":"execute_result","execution_count":114,"data":{"text/plain":"          0     1    2      3      4      5      6      7      8      9   ...  \\\n99  share_49  2000  FEB  215.1  217.0  216.2  247.7  235.1  221.4  251.3  ...   \n\n       26     27     28     29     30     31   32    33   34   35  \n99  238.4  232.5  236.5  229.6  252.3  227.8  1.0  20.0  0.0  0.0  \n\n[1 rows x 36 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>34</th>\n      <th>35</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>99</th>\n      <td>share_49</td>\n      <td>2000</td>\n      <td>FEB</td>\n      <td>215.1</td>\n      <td>217.0</td>\n      <td>216.2</td>\n      <td>247.7</td>\n      <td>235.1</td>\n      <td>221.4</td>\n      <td>251.3</td>\n      <td>...</td>\n      <td>238.4</td>\n      <td>232.5</td>\n      <td>236.5</td>\n      <td>229.6</td>\n      <td>252.3</td>\n      <td>227.8</td>\n      <td>1.0</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 36 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Our Dataset don't have same length of share price for all the records because of different days in each month, RNN will expect to have same size of records so we need to preprocess to generate consistence records."},{"metadata":{"trusted":true},"cell_type":"code","source":"def targetDateReplacer(train_data):\n    '''\n    Replace buy date and sell date with zeros so that we can generate consitent data \n    '''\n    df = pd.DataFrame(columns = [0,1,2,3,4,5,6,7,8,9,10,11,12,13, 14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35])\n    for index,row in enumerate(train_data.values):\n        days_in_month = monthrange(int(row[1]), int(month_dict[row[2]]))[1]\n        sell_index = days_in_month+4\n        buy_index = days_in_month+3\n        row[sell_index] = 0\n        row[buy_index] = 0\n        df.loc[index]=row\n    return df\n\n#modified_df = targetDateReplacer(train_data)\nmodified_df.head()","execution_count":115,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-115-c1e568776070>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodified_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargetDateReplacer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mmodified_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-115-c1e568776070>\u001b[0m in \u001b[0;36mtargetDateReplacer\u001b[0;34m(train_data)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msell_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbuy_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;31m# set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer_missing\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    649\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   7116\u001b[0m                 \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7117\u001b[0m                 \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7118\u001b[0;31m                 \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcombined_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7119\u001b[0m             )\n\u001b[1;32m   7120\u001b[0m             \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;31m# For data is list-like, or Iterable (will consume into list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_ndarray\u001b[0;34m(values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mblock_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m         \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m         \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_known_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_consolidate\u001b[0;34m(blocks)\u001b[0m\n\u001b[1;32m   1906\u001b[0m     \u001b[0;31m# sort by _can_consolidate, dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1907\u001b[0m     \u001b[0mgkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1908\u001b[0;31m     \u001b[0mgrouper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m     \u001b[0mnew_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1906\u001b[0m     \u001b[0;31m# sort by _can_consolidate, dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1907\u001b[0;31m     \u001b[0mgkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1908\u001b[0m     \u001b[0mgrouper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m_consolidate_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_consolidate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/_dtype.py\u001b[0m in \u001b[0;36m_name_get\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_name_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m     \u001b[0;31m# provides dtype.name.__get__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"modified_df=modified_df.drop([34,35], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we need to one hot encode sharetype, year, and month column\n\nfrom sklearn.preprocessing import LabelEncoder\ndef label_encoder(modified_df):\n    cols = (0, 1, 2)\n    # process columns, apply LabelEncoder to categorical features\n    for c in cols:\n        lbl = LabelEncoder() \n        lbl.fit(list(modified_df[c].values)) \n        modified_df[c] = lbl.transform(list(modified_df[c].values))\n    \n    return modified_df\n\none_hot_df=label_encoder(modified_df)\none_hot_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one hot encoded value of first column\nmodified_df[0].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one hot encoded value of second column\nmodified_df[1].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one hot encoded value of third column\nmodified_df[2].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Normalization ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\ndef normalize_data(df):\n    min_max_scaler = preprocessing.MinMaxScaler()\n    for i in range(34):\n        df[i] = min_max_scaler.fit_transform(df[i].values.reshape(-1,1))\n    return df\n\nnorm_df=normalize_data(modified_df)\nnorm_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"norm_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we also need to normalize target values\nmin_max_scaler = preprocessing.MinMaxScaler()\nn_buy_dates = min_max_scaler.fit_transform(np.asarray(buy_dates).reshape(-1,1))\nn_sell_dates = min_max_scaler.fit_transform(np.asarray(sell_dates).reshape(-1,1))\n\n# lets create target value using list comprehension \nn_buy_dates=list(n_buy_dates)\nn_sell_dates=list(n_sell_dates)\ntarget=[[n_buy_dates[i].item(),n_sell_dates[i].item()] for i in range(len(n_sell_dates))]\ntarget[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split_dataset into 80% training , 10% test and 10% Validation Dataset\ntrain_x=np.array(norm_df[:int(0.90*len(norm_df))])\ntrain_y=np.array(target[:int(0.90*len(target))])\nvalid_x=np.array(norm_df[int(0.90*len(norm_df)):])\nvalid_y=np.array(target[int(0.90*len(target)):])\nprint(len(train_y), len(train_x), len(valid_y), len(valid_x))","execution_count":89,"outputs":[{"output_type":"stream","text":"10970 10970 1219 1219\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = np.reshape(train_x, (train_x.shape[0], 1, train_x.shape[1]))\nvalid_x = np.reshape(valid_x, (valid_x.shape[0], 1, valid_x.shape[1]))","execution_count":97,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x.shape","execution_count":98,"outputs":[{"output_type":"execute_result","execution_count":98,"data":{"text/plain":"(10970, 1, 34)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_features=34\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM\n\nmodel = Sequential()\nmodel.add(LSTM(100, input_shape=(1, n_features)))\nmodel.add(Dense(2))\nmodel.compile(optimizer='adam', loss='mse')\nmodel.summary()","execution_count":104,"outputs":[{"output_type":"stream","text":"Model: \"sequential_9\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_7 (LSTM)                (None, 100)               54000     \n_________________________________________________________________\ndense_7 (Dense)              (None, 2)                 202       \n=================================================================\nTotal params: 54,202\nTrainable params: 54,202\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train_x, train_y, epochs=10, validation_split=0.2, batch_size=50)","execution_count":105,"outputs":[{"output_type":"stream","text":"Train on 8776 samples, validate on 2194 samples\nEpoch 1/10\n8776/8776 [==============================] - 1s 139us/step - loss: 0.0864 - val_loss: 0.0665\nEpoch 2/10\n8776/8776 [==============================] - 1s 61us/step - loss: 0.0642 - val_loss: 0.0654\nEpoch 3/10\n8776/8776 [==============================] - 1s 61us/step - loss: 0.0631 - val_loss: 0.0645\nEpoch 4/10\n8776/8776 [==============================] - 1s 63us/step - loss: 0.0628 - val_loss: 0.0655\nEpoch 5/10\n8776/8776 [==============================] - 1s 62us/step - loss: 0.0624 - val_loss: 0.0652\nEpoch 6/10\n8776/8776 [==============================] - 1s 63us/step - loss: 0.0622 - val_loss: 0.0639\nEpoch 7/10\n8776/8776 [==============================] - 1s 63us/step - loss: 0.0623 - val_loss: 0.0639\nEpoch 8/10\n8776/8776 [==============================] - 1s 62us/step - loss: 0.0620 - val_loss: 0.0653\nEpoch 9/10\n8776/8776 [==============================] - 1s 61us/step - loss: 0.0622 - val_loss: 0.0664\nEpoch 10/10\n8776/8776 [==============================] - 1s 62us/step - loss: 0.0622 - val_loss: 0.0637\n","name":"stdout"},{"output_type":"execute_result","execution_count":105,"data":{"text/plain":"<keras.callbacks.callbacks.History at 0x7f4ccc069fd0>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Pytorch ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\n#create Tensor Dataset\ntrain_data=TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\nvalid_data=TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n\n#dataloader\nbatch_size=50\ntrain_loader=DataLoader(train_data, batch_size=batch_size, shuffle=True)\nvalid_loader=DataLoader(valid_data, batch_size=batch_size, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# obtain one batch of training data\ndataiter = iter(train_loader)\nsample_x, sample_y = dataiter.next()\nprint('Sample input size: ', sample_x.size()) # batch_size, seq_length\nprint('Sample input: \\n', sample_x)\nprint()\nprint('Sample label size: ', sample_y.size()) # batch_size\nprint('Sample label: \\n', sample_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\n\nclass RNN(nn.Module):\n    \"\"\"\n    The RNN model that will be used to perform Sentiment analysis.\n    \"\"\"\n\n    def __init__(self, output_size, input_dim, hidden_dim, n_layers, drop_prob=0.5):\n        \"\"\"\n        Initialize the model by setting up the layers.\n        \"\"\"\n        super(RNN, self).__init__()\n\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        self.input_dim = input_dim\n        \n        # embedding and LSTM layers\n        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, \n                            dropout=drop_prob, batch_first=True)\n        \n        # dropout layer\n        self.dropout = nn.Dropout(0.3)\n        \n        # linear and sigmoid layers\n        self.fc1 = nn.Linear(hidden_dim, 256)\n        self.fc2 = nn.Linear(256, 32)\n        self.fc3 = nn.Linear(32, output_size)\n        self.sig = nn.Sigmoid()\n        \n\n    def forward(self, x, hidden):\n        \"\"\"\n        Perform a forward pass of our model on some input and hidden state.\n        \"\"\"\n        \n        \n        batch_size = x.size(0)\n        print(batch_size)\n        \n        x = x.unsqueeze(0)\n        print(x, x.shape)\n        \n        lstm_out, hidden = self.lstm(x, hidden)\n    \n        # stack up lstm outputs\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        # dropout and fully-connected layer\n        out = self.dropout(lstm_out)\n        out = self.fc1(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n\n        \n        # return last sigmoid output and hidden state\n        return out, hidden\n    \n    \n    def init_hidden(self, batch_size):\n        ''' Initializes hidden state '''\n        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n        # initialized to zero, for hidden state and cell state of LSTM\n        weight = next(self.parameters()).data\n        \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n        \n        return hidden","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate the model w/ hyperparams\noutput_size = 2\ninput_dim = 34\nhidden_dim = 100\nn_layers = 2\n\nnet = RNN(output_size, input_dim, hidden_dim, n_layers)\n\nprint(net)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(list(net.parameters()))):\n    print(list(net.parameters())[i].size())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loss and optimization functions\nlr=0.001\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First checking if GPU is available\ntrain_on_gpu=torch.cuda.is_available()\n\nif(train_on_gpu):\n    print('Training on GPU.')\nelse:\n    print('No GPU available, training on CPU.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training params\n\nepochs = 2 # 3-4 is approx where I noticed the validation loss stop decreasing\n\ncounter = 0\nprint_every = 100\nclip=5 # gradient clipping\n# move model to GPU, if available\nif(train_on_gpu):\n    net.cuda()\n\nnet.train()\n# train for some number of epochs\nfor e in range(epochs):\n    # initialize hidden state\n    h = net.init_hidden(50)\n    # batch loop\n    for inputs, labels in train_loader:\n        counter += 1    \n        if(train_on_gpu):\n            inputs, labels = inputs.cuda(), labels.cuda()\n        \n        # Creating new variables for the hidden state, otherwise\n        # we'd backprop through the entire training history\n        h = tuple([each.data for each in h])\n        # zero accumulated gradients\n        net.zero_grad()\n\n        # get the output from the model\n        output, h = net(inputs, h)\n\n        # calculate the loss and perform backprop\n        loss = criterion(output.squeeze(), labels.float())\n        loss.backward()\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n        nn.utils.clip_grad_norm_(net.parameters(), clip)\n        optimizer.step()\n\n        # loss stats\n        if counter % print_every == 0:\n            # Get validation loss\n            val_h = net.init_hidden(batch_size)\n            val_losses = []\n            net.eval()\n            for inputs, labels in valid_loader:\n                # Creating new variables for the hidden state, otherwise\n                # we'd backprop through the entire training history\n                val_h = tuple([each.data for each in val_h])\n                if(inputs.shape[0] != batch_size):\n                    continue\n                if(train_on_gpu):\n                    inputs, labels = inputs.cuda(), labels.cuda()\n\n                output, val_h = net(inputs, val_h)\n                val_loss = criterion(output.squeeze(), labels.float())\n\n                val_losses.append(val_loss.item())\n\n            net.train()\n            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n                  \"Step: {}...\".format(counter),\n                  \"Loss: {:.6f}...\".format(loss.item()),\n                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":93,"outputs":[{"output_type":"stream","text":"Model: \"sequential_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_3 (LSTM)                (None, 100)               54000     \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 101       \n=================================================================\nTotal params: 54,101\nTrainable params: 54,101\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":87,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Error when checking input: expected lstm_2_input to have 3 dimensions, but got array with shape (10970, 34)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-87-9dfc21002c2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    133\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Error when checking input: expected lstm_2_input to have 3 dimensions, but got array with shape (10970, 34)"]}]},{"metadata":{},"cell_type":"markdown","source":"Testing Data (share_test_data.csv)\n1. Predict buy and sell date for estimated shares prices for next 2 years.\n2. You can only buy once in month and sell once in month for given share.\n3. Testing data have same format as training data except buy and sell date are not present."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv('/kaggle/input/sharepricedateprediction/share_test_data.csv', header=None)\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"en_test=label_encoder(test_data)\nnorm_data=normalize_data(en_test)\nnorm_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Expected Output\n1. Python code for training model using given train data using Tensorflow, Pytorch or Keras frameworks.\n2. Python code for predicting on test data using trained model.\n3. Generated predictions for test data in same format as training data. \n4. A PDF report explaining your approach and results. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}